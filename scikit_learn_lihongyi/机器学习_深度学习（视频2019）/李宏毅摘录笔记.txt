TensorFlow2.0 20190308/20191001

全世界最为流行的深度学习框架有PaddlePaddle、Tensorflow、Caffe、Theano、MXNet、Torch和PyTorch。
TF+Keras
PyThroch+Caffe2

P08 error come from bias and variance
Bias 指标衡量了在训练阶段，机器学习算法和真实数据之间的差异。偏差程度越大，bias 越大。(欠拟合underfitting)
Variance 表示在不同测试集间，预测效果间的偏差程度，偏差程度越大，variance 越大。（过拟合overfitting）

简单的model：bias平均误差比较大，variance方差误差比较小。
复杂的model：bias平均误差比较小，variance方差误差比较大。


数据增强：旋转+平移+最小二乘变形
正则化技术：


深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。
批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）




P09 Classification
概率论和数理统计
perceptron感知机
SVM支持向量机
SVR支持向量回归

probability distribution 概率分布
概率分布有两种类型：离散（discrete）概率分布和连续（continuous）概率分布。
离散概率分布也称为概率质量函数（probability mass function）。离散概率分布的例子有伯努利分布（Bernoulli distribution）、二项分布（binomial distribution）、泊松分布（Poisson distribution）和几何分布（geometric distribution）等。
连续概率分布也称为概率密度函数（probability density function），它们是具有连续取值（例如一条实线上的值）的函数。正态分布（normal distribution）、指数分布（exponential distribution）和β分布（beta distribution）等都属于连续概率分布。

Gaussian Distribution高斯分布（正态分布）
正态分布是一种概率分布。正态分布是具有两个参数μ和σ2的连续型随机变量的分布第一参数μ是遵从正态分布的随机变量的均值，第二个参数σ2是此随机变量的方差。

bernoulli distribution 伯努利分布

Maximum Likelihood Estimation 最大似然估计
naive Bayes classifier 朴素贝叶斯分类器


P10 Logistic Regression
square error 均方差
cross entropy 交叉熵
生成模型（Generative）和判别模型（Discriminative）
生成模型（代表：朴素贝叶斯，隐马尔科夫模型）和判别模型（代表：k近邻法、感知机、决策树、逻辑回归、最大熵、SVM、AdaBoost和条件随机场模型）



P17 集成模型方法Ensemble（bagging、boosting、stacking）
自助法、自助聚合（bagging）、随机森林、提升法（boosting）、堆叠法（stacking）以及许多其它的基础集成学习模型。
基本模型本身的性能并不是非常好，这要么是因为它们具有较高的偏置（例如，低自由度模型），要么是因为他们的方差太大导致鲁棒性不强（例如，高自由度模型）。即欠拟合underfitting(high bias)和过拟合overfitting(high variance)
集成方法的思想是通过将这些弱学习器的偏置和/或方差结合起来，从而创建一个「强学习器」（或「集成模型」），从而获得更好的性能。

粗略概述：bagging主要用于解决高方差high variance过拟合overfitting，boosting、stacking主要用于解决高偏置high bias欠拟合underfitting。

鞍点saddle poin 一个不是局部极值点的驻点称为鞍点。
黑塞矩阵Hessian矩阵是标量值函数或标量场函数的二阶偏导数的方块矩阵。它描述了许多变量函数的局部曲率，可以用于判定多元函数的极值。
一个简单标准的方法验证一个静止点是否为一个实数函数的鞍点，就是计算该函数的在该点上的 Hessian 矩阵。如果该 Hessian 矩阵为不定的，则该点为该函数的鞍点。




深度学习-自学习（self taught learning）
自学习与半监督学习：两者都是有少量的标注数据和大量的未标注数据的情况。两者的不同在于半监督学习要求标注数据和未标注数据有同样的分布，而自学习没有这种要求，因此具有更广泛的应用。


非监督学习：
小样本学习
http://www.dataguru.cn/article-14644-1.html
https://blog.csdn.net/weixin_37589575/article/details/92801610
Clustering
PCA（Principle Component Analysis）即主成分分析法，是特征降维的最常用手段。顾名思义，PCA 能从冗余特征中提取主要成分，在不太损失模型质量的情况下，提升了模型训练速度。
PCA
https://www.jianshu.com/p/c486b0c5e506
https://blog.csdn.net/program_developer/article/details/80632779
https://blog.csdn.net/Tonywu2018/article/details/89158687
Word Embedding
https://www.jianshu.com/p/af8f20fe7dd3
LDA
https://blog.csdn.net/u011808673/article/details/82497195
CCA
https://blog.csdn.net/changyuanchn/article/details/81293994
Matrix Factorization
https://www.jianshu.com/p/60639fb8590e
NMF
https://www.jianshu.com/p/ac338668ba44
K-Means
https://www.jianshu.com/p/4f032dccdcef
https://blog.csdn.net/sinat_36710456/article/details/88019323
Hierarchical Clustering
https://blog.csdn.net/qq_39388410/article/details/78240037
variance vs covariance
https://blog.csdn.net/weixin_40920228/article/details/80574332
https://www.cnblogs.com/leezx/p/9929340.html
t-SNE

PCA是特征降维的最常用手段。主要是通过对协方差矩阵进行特征分解，以得出数据的主成分（即特征向量）与它们的权值（即特征值）。
SVD(Singular Value Decomposition)分解，也叫奇异值分解。


Face Verification
验证(Verification)与确认（Validation）
meta learning
siamese network
prototypical network
matching network
relation network


auto-encoder
TF/IDF
bottle-neck layer
VAE
GAN


强化深度学习
Deep Reinforcement Learning
Policy-based (Learning an Actor)
Policy Gradient
Pathwise Derivative Policy Gradient
Value-based(Learning an Critic)
Asynchronous Advantage Actor- critic (A3C)
Double DQN
Dueling DQN
Inverse Reinforcement Learning  (Expert+Environments ===> Reward)


network compression
network pruning
knowledge distillation

Transformer
Self-Attention
ELMO
BERT
GPT
natural language inference
scratch
https://talktotransformer.com/


Auto-regressive Model
VAE(Variational Auto-encoder)
GAN(Generative Adversarial Network)
Flow-base Model
Jacobian Matrix
Determinant
Change of Variable Theorem
Coupling Layer

MATLAB




https://www.jianshu.com/p/49766c5fe130
https://openai.com/blog/generative-models/


